{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # Generating Responses from GSM8K\n",
    "\n",
    "\n",
    "\n",
    " This notebook generates and saves model responses from the GSM8K dataset.\n",
    "\n",
    " We'll generate both thinking and non-thinking responses to create our paired dataset\n",
    "\n",
    " for identifying reasoning length direction.\n",
    "\n",
    "\n",
    "\n",
    " The notebook follows these steps:\n",
    "\n",
    " 1. Load the GSM8K dataset\n",
    "\n",
    " 2. Generate responses with thinking enabled\n",
    "\n",
    " 3. Generate responses with thinking disabled\n",
    "\n",
    " 4. Save the paired responses for later analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Setup\n",
    "\n",
    "\n",
    "\n",
    " First, let's import the necessary libraries and set up the argument parser."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/manikyabardhan/miniforge3/envs/steer/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python version: 3.11.0 | packaged by conda-forge | (main, Jan 14 2023, 12:26:40) [Clang 14.0.6 ]\n",
      "PyTorch version: 2.7.0\n",
      "Transformers version: 4.51.3\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import torch\n",
    "import argparse\n",
    "from tqdm import tqdm\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "import transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Command Line Arguments\n",
    "\n",
    "\n",
    "\n",
    " When running this as a script, you can provide command-line arguments.\n",
    "\n",
    " In notebook mode, we'll define default values that you can modify in the next cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_args():\n",
    "    parser = argparse.ArgumentParser(\n",
    "        description=\"Generate responses from GSM8K dataset\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--model\", type=str, default=\"Qwen/Qwen3-0.6B\", help=\"Model name or path\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--output_dir\",\n",
    "        type=str,\n",
    "        default=\"responses\",\n",
    "        help=\"Directory to save responses\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--num_samples\",\n",
    "        type=int,\n",
    "        default=100,\n",
    "        help=\"Number of samples to process from GSM8K\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--max_new_tokens\",\n",
    "        type=int,\n",
    "        default=1024,\n",
    "        help=\"Maximum number of new tokens to generate\",\n",
    "    )\n",
    "    parser.add_argument(\"--seed\", type=int, default=42, help=\"Random seed\")\n",
    "    return parser.parse_args()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Interactive Configuration\n",
    "\n",
    "\n",
    "\n",
    " If you're running this as a notebook, you can modify these parameters directly.\n",
    "\n",
    " Change the values in this cell to customize your experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running in notebook mode with these parameters:\n",
      "- Model: Qwen/Qwen3-0.6B\n",
      "- Number of samples: 5\n",
      "- Output directory: responses\n"
     ]
    }
   ],
   "source": [
    "# Interactive notebook parameters - modify these values as needed\n",
    "class NotebookArgs:\n",
    "    def __init__(self):\n",
    "        self.model = \"Qwen/Qwen3-0.6B\"  # Model to use\n",
    "        self.output_dir = \"responses\"  # Directory to save responses\n",
    "        self.num_samples = 5  # Use a small number for quick testing\n",
    "        self.max_new_tokens = 1024  # Maximum new tokens to generate\n",
    "        self.seed = 42  # Random seed for reproducibility\n",
    "\n",
    "\n",
    "# Use NotebookArgs when running as notebook, otherwise parse command line arguments\n",
    "import sys\n",
    "\n",
    "if \"ipykernel\" in sys.modules:\n",
    "    args = NotebookArgs()\n",
    "    print(\"Running in notebook mode with these parameters:\")\n",
    "    print(f\"- Model: {args.model}\")\n",
    "    print(f\"- Number of samples: {args.num_samples}\")\n",
    "    print(f\"- Output directory: {args.output_dir}\")\n",
    "else:\n",
    "    args = parse_args()\n",
    "    print(\"Running in script mode with parsed arguments\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Load and Prepare the GSM8K Dataset\n",
    "\n",
    "\n",
    "\n",
    " Now we'll load the GSM8K dataset and take a subset for our experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_gsm8k_dataset(num_samples, seed=42):\n",
    "    \"\"\"Load and prepare the GSM8K dataset.\"\"\"\n",
    "    dataset = load_dataset(\"gsm8k\", \"main\")\n",
    "\n",
    "    # Use the train split and take a subset\n",
    "    train_data = dataset[\"train\"].shuffle(seed=seed).select(range(num_samples))\n",
    "\n",
    "    return train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating train split: 100%|██████████| 7473/7473 [00:00<00:00, 555607.36 examples/s]\n",
      "Generating test split: 100%|██████████| 1319/1319 [00:00<00:00, 393029.77 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 5 examples from GSM8K\n",
      "\n",
      "Example problem:\n",
      "--------------------------------------------------\n",
      "Question: Mimi picked up 2 dozen seashells on the beach.  Kyle found twice as many shells as Mimi and put them in his pocket. Leigh grabbed one-third of the shells that Kyle found.  How many seashells did Leigh have?\n",
      "Answer: Mimi has 2 x 12 = <<2*12=24>>24 sea shells.\n",
      "Kyle has 24 x 2 = <<24*2=48>>48 sea shells.\n",
      "Leigh has 48 / 3 = <<48/3=16>>16 sea shells.\n",
      "#### 16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Let's load the dataset and examine a sample\n",
    "dataset = load_gsm8k_dataset(args.num_samples, args.seed)\n",
    "print(f\"Loaded {len(dataset)} examples from GSM8K\")\n",
    "\n",
    "# Display an example\n",
    "if len(dataset) > 0:\n",
    "    example = dataset[0]\n",
    "    print(\"\\nExample problem:\")\n",
    "    print(\"-\" * 50)\n",
    "    print(f\"Question: {example['question']}\")\n",
    "    print(f\"Answer: {example['answer']}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Generate Responses\n",
    "\n",
    "\n",
    "\n",
    " Now let's define a function to generate responses from our model with and without thinking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_response(model, tokenizer, question, enable_thinking=True):\n",
    "    \"\"\"Generate a response from the model with or without thinking.\"\"\"\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": f\"Solve this math problem step by step:\\n{question}\",\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    text = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True,\n",
    "        enable_thinking=enable_thinking,  # Toggle thinking mode\n",
    "    )\n",
    "\n",
    "    # Let's see what the prompt looks like (for debugging)\n",
    "    if enable_thinking:\n",
    "        print(f\"Prompt with thinking enabled (first 100 chars): {text[:100]}...\")\n",
    "\n",
    "    inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        generated_ids = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=args.max_new_tokens,\n",
    "            do_sample=False,  # Use greedy decoding for deterministic outputs\n",
    "        )\n",
    "\n",
    "    output_ids = generated_ids[0][len(inputs.input_ids[0]) :].tolist()\n",
    "\n",
    "    # Parse thinking content if applicable\n",
    "    if enable_thinking:\n",
    "        try:\n",
    "            # Find index of </think> token\n",
    "            think_end_token = tokenizer.encode(\"</think>\", add_special_tokens=False)[-1]\n",
    "            think_end_index = (\n",
    "                output_ids.index(think_end_token)\n",
    "                if think_end_token in output_ids\n",
    "                else -1\n",
    "            )\n",
    "\n",
    "            if think_end_index != -1:\n",
    "                thinking_content = tokenizer.decode(\n",
    "                    output_ids[:think_end_index], skip_special_tokens=True\n",
    "                ).strip()\n",
    "                content = tokenizer.decode(\n",
    "                    output_ids[think_end_index + 1 :], skip_special_tokens=True\n",
    "                ).strip()\n",
    "                return {\"thinking\": thinking_content, \"response\": content}\n",
    "        except ValueError:\n",
    "            pass\n",
    "\n",
    "        # If no thinking token found or error occurred, return everything as response\n",
    "        content = tokenizer.decode(output_ids, skip_special_tokens=True).strip()\n",
    "        return {\"thinking\": \"\", \"response\": content}\n",
    "    else:\n",
    "        content = tokenizer.decode(output_ids, skip_special_tokens=True).strip()\n",
    "        return {\"thinking\": \"\", \"response\": content}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Test the Model on a Single Example\n",
    "\n",
    "\n",
    "\n",
    " Let's first test the model on a single example to make sure everything is working properly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model Qwen/Qwen3-0.6B...\n"
     ]
    }
   ],
   "source": [
    "print(f\"Loading model {args.model}...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(args.model)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    args.model, torch_dtype=\"auto\", device_map=\"auto\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test with a simple problem\n",
    "test_question = \"If there are 5 apples and 3 are eaten, how many remain?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generating response with thinking enabled...\n",
      "Prompt with thinking enabled (first 100 chars): <|im_start|>user\n",
      "Solve this math problem step by step:\n",
      "If there are 5 apples and 3 are eaten, how ma...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/manikyabardhan/miniforge3/envs/steer/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/Users/manikyabardhan/miniforge3/envs/steer/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.95` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "/Users/manikyabardhan/miniforge3/envs/steer/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:653: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `20` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Thinking content:\n",
      "--------------------------------------------------\n",
      "<think>\n",
      "Okay, let's see. The problem says there are 5 apples and 3 are eaten. I need to figure out how many remain. Hmm, so first, I should probably start by counting the apples. There are 5 apples in total. Then, 3 are eaten. So, if I subtract the number of apples eaten from the total, that should give me the remaining apples. Let me write that down to make sure I don't make a mistake.\n",
      "\n",
      "Total apples = 5. Apples eaten = 3. So, remaining apples = Total apples - Apples eaten. That would be 5 - 3. Let me do the subtraction. 5 minus 3 is 2. So, there are 2 apples left. Wait, is there anything else I need to consider here? Like, maybe the apples are eaten in a different way? But the problem doesn't mention anything about the apples being eaten in groups or anything else. It just says 3 are eaten. So, I think subtracting them is the right approach. \n",
      "\n",
      "Let me double-check. If there were 5 apples, and 3 are taken out, then yes, 5 minus 3 is 2. So, the answer should be 2. I don't think I made any mistakes here. It seems straightforward. Maybe I can visualize it with numbers. Imagine 5 apples: 1, 2, 3, 4, 5. If I take away 3, then it's 2, 3, 4, 5. So, only 2 is left. Yep, that makes sense. I think that's it. No need for more steps here. The problem is pretty simple.\n",
      "\n",
      "Response content:\n",
      "--------------------------------------------------\n",
      "To solve the problem:\n",
      "\n",
      "- There are **5 apples** in total.\n",
      "- **3 apples are eaten**.\n",
      "\n",
      "To find the number of apples remaining:\n",
      "\n",
      "$$\n",
      "\\text{Remaining apples} = \\text{Total apples} - \\text{Apples eaten} = 5 - 3 = 2\n",
      "$$\n",
      "\n",
      "**Answer:** There are 2 apples remaining. \n",
      "\n",
      "$$\n",
      "\\boxed{2}\n",
      "$$\n"
     ]
    }
   ],
   "source": [
    "# With thinking enabled\n",
    "print(\"\\nGenerating response with thinking enabled...\")\n",
    "thinking_result = generate_response(\n",
    "    model, tokenizer, test_question, enable_thinking=True\n",
    ")\n",
    "\n",
    "print(\"\\nThinking content:\")\n",
    "print(\"-\" * 50)\n",
    "print(thinking_result[\"thinking\"])\n",
    "\n",
    "print(\"\\nResponse content:\")\n",
    "print(\"-\" * 50)\n",
    "print(thinking_result[\"response\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generating response with thinking disabled...\n",
      "\n",
      "Response content (no thinking):\n",
      "--------------------------------------------------\n",
      "We are given:\n",
      "\n",
      "- **5 apples** in total  \n",
      "- **3 apples are eaten**\n",
      "\n",
      "We need to find how many **remain**.\n",
      "\n",
      "### Step-by-step:\n",
      "\n",
      "1. Start with the total number of apples:  \n",
      "   $ 5 $\n",
      "\n",
      "2. Subtract the number of apples eaten:  \n",
      "   $ 5 - 3 = 2 $\n",
      "\n",
      "### Final Answer:\n",
      "$$\n",
      "\\boxed{2}\n",
      "$$\n",
      "\n",
      "There are **2 apples** remaining.\n"
     ]
    }
   ],
   "source": [
    "# With thinking disabled\n",
    "print(\"\\nGenerating response with thinking disabled...\")\n",
    "non_thinking_result = generate_response(\n",
    "    model, tokenizer, test_question, enable_thinking=False\n",
    ")\n",
    "\n",
    "print(\"\\nResponse content (no thinking):\")\n",
    "print(\"-\" * 50)\n",
    "print(non_thinking_result[\"response\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Main Function\n",
    "\n",
    "\n",
    "\n",
    " Now let's define the main function to process the whole dataset and save the responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(args):\n",
    "    \"\"\"Main function to process GSM8K examples and save paired responses.\"\"\"\n",
    "    # Create output directory if it doesn't exist\n",
    "    os.makedirs(args.output_dir, exist_ok=True)\n",
    "    model_short_name = args.model.split(\"/\")[-1]\n",
    "\n",
    "    # Load model and tokenizer\n",
    "    print(f\"Loading model {args.model}...\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(args.model)\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        args.model, torch_dtype=\"auto\", device_map=\"auto\"\n",
    "    )\n",
    "\n",
    "    # Load dataset\n",
    "    print(f\"Loading GSM8K dataset...\")\n",
    "    dataset = load_gsm8k_dataset(args.num_samples, args.seed)\n",
    "\n",
    "    # Generate and save responses\n",
    "    outputs = []\n",
    "    for i, example in enumerate(tqdm(dataset, desc=\"Generating responses\")):\n",
    "        question = example[\"question\"]\n",
    "\n",
    "        # Generate with thinking enabled\n",
    "        thinking_result = generate_response(\n",
    "            model, tokenizer, question, enable_thinking=True\n",
    "        )\n",
    "\n",
    "        # Generate with thinking disabled\n",
    "        non_thinking_result = generate_response(\n",
    "            model, tokenizer, question, enable_thinking=False\n",
    "        )\n",
    "\n",
    "        # Save the results\n",
    "        output = {\n",
    "            \"id\": i,\n",
    "            \"question\": question,\n",
    "            \"answer\": example[\"answer\"],\n",
    "            \"with_thinking\": thinking_result,\n",
    "            \"without_thinking\": non_thinking_result,\n",
    "        }\n",
    "        outputs.append(output)\n",
    "\n",
    "    # Save all responses to a JSON file\n",
    "    output_path = os.path.join(\n",
    "        args.output_dir, f\"{model_short_name}_gsm8k_responses.json\"\n",
    "    )\n",
    "    with open(output_path, \"w\") as f:\n",
    "        json.dump(outputs, f, indent=2)\n",
    "\n",
    "    print(f\"Responses saved to {output_path}\")\n",
    "    return outputs\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Execute the Main Function\n",
    "\n",
    "\n",
    "\n",
    " Let's run our main function to generate and save the paired responses.\n",
    "\n",
    " This might take a while depending on the number of samples and the model size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute the main function when running as a script or if explicitly requested\n",
    "if __name__ == \"__main__\" or \"ipykernel\" in sys.modules:\n",
    "    if \"ipykernel\" in sys.modules:\n",
    "        print(\"Running in notebook mode, processing a few examples...\")\n",
    "        # Use a smaller number of samples for interactive testing\n",
    "        args.num_samples = min(args.num_samples, 5)\n",
    "\n",
    "    outputs = main(args)\n",
    "\n",
    "    # In notebook mode, let's also examine the first saved response\n",
    "    if \"ipykernel\" in sys.modules and outputs and len(outputs) > 0:\n",
    "        print(\"\\nExample of a saved response pair:\")\n",
    "        print(\"-\" * 50)\n",
    "        print(f\"Question: {outputs[0]['question']}\")\n",
    "        print(f\"\\nThinking: {outputs[0]['with_thinking']['thinking'][:200]}...\")\n",
    "        print(\n",
    "            f\"\\nResponse (with thinking): {outputs[0]['with_thinking']['response'][:200]}...\"\n",
    "        )\n",
    "        print(\n",
    "            f\"\\nResponse (without thinking): {outputs[0]['without_thinking']['response'][:200]}...\"\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Next Steps\n",
    "\n",
    "\n",
    "\n",
    " Now that we've generated paired responses with and without thinking, we can use this data to extract the reasoning length direction.\n",
    "\n",
    "\n",
    "\n",
    " Continue to the next notebook: `extract_reasoning_length_direction.py`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "steer",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
