# Response Analysis Tool

This script analyzes response files generated by `generate_responses_gsm8k.py` and creates tables showing model performance, particularly focusing on the accuracy and reasoning length of the shortest thinking responses.

## Features

The script generates three main tables:

1. **Overall Accuracy Table**: Shows overall accuracy, total responses, and average thinking length for each model
2. **Accuracy of Shortest Reasoning Responses**: Shows accuracy for the top 5%, 10%, and 20% shortest reasoning responses
3. **Average Reasoning Length for Shortest Responses**: Shows average token count and statistics for the shortest reasoning responses

## Key Features

- **Automatic Processing**: Automatically processes raw response files using `postprocess_responses.py` if needed
- **Multiple Format Support**: Handles both processed and unprocessed response files
- **Robust Token Counting**: Uses actual tokenizer when available, falls back to approximation
- **Comprehensive Analysis**: Saves detailed analysis, examples, and CSV tables
- **Thinking Text Extraction**: Intelligently extracts thinking text from various formats

## Usage

### Basic Usage

```bash
python analyze_response_tables.py \
    --response_files responses/model1_responses.json responses/model2_responses.json \
    --model_names "Model1" "Model2"
```

### Example with Available Files

```bash
# Compare Qwen3-4B with and without ThinkEdit
python analyze_response_tables.py \
    --response_files responses/Qwen3-4B_gsm8k_responses.json responses/qwen3-4B-thinkedit_gsm8k_responses.json \
    --model_names "Qwen3-4B-Original" "Qwen3-4B-ThinkEdit" \
    --save_csv \
    --verbose
```

```bash
# Compare multiple model sizes
python analyze_response_tables.py \
    --response_files responses/Qwen3-0.6B_gsm8k_responses.json responses/Qwen3-4B_gsm8k_responses.json responses/qwen3_8b_gsm8k_responses.json \
    --model_names "Qwen3-0.6B" "Qwen3-4B" "Qwen3-8B" \
    --tokenizer_model "Qwen/Qwen3-0.6B" \
    --output_dir "model_comparison_results"
```

### Quick Start Example

```bash
# Run the example script
python example_analysis.py
```

## Arguments

- `--response_files`: List of response JSON files to analyze (required)
- `--model_names`: Custom names for models (optional, defaults to filenames)
- `--tokenizer_model`: Model to use for tokenization (default: "Qwen/Qwen3-0.6B")
- `--output_dir`: Directory to save detailed analysis (default: "analysis_results")
- `--save_csv`: Save tables as CSV files
- `--verbose`: Enable verbose logging
- `--force_reprocess`: Force reprocessing even if processed files exist

## Output

### Console Output
The script prints three formatted tables to the console:

1. **OVERALL ACCURACY TABLE**
2. **ACCURACY OF SHORTEST REASONING RESPONSES** 
3. **AVERAGE REASONING LENGTH FOR SHORTEST RESPONSES**

### File Output
- Detailed JSON analysis for each model: `{model_name}_detailed_analysis.json`
- Example responses for each percentile: `{model_name}_top_{5|10|20}%_examples.json`
- CSV files (if --save_csv is used):
  - `overall_accuracy.csv`
  - `shortest_responses_accuracy.csv` 
  - `shortest_responses_length.csv`

## Example Results

### Overall Accuracy Table
```
Model               Total Responses  Overall Accuracy (%)  Correct Count  Avg Thinking Length (tokens)
Qwen3-4B-Original             2000                 94.95           1899                       1495.0
Qwen3-4B-ThinkEdit            2000                 95.35           1907                       1493.6
```

### Accuracy of Shortest Reasoning Responses
```
Model               5% Shortest Accuracy (%)  10% Shortest Accuracy (%)  20% Shortest Accuracy (%)
Qwen3-4B-Original                     100.00                      100.00                      99.75
Qwen3-4B-ThinkEdit                    100.00                      100.00                      99.75
```

### Average Reasoning Length for Shortest Responses
```
Model               5% Avg Length  5% Count  10% Avg Length  10% Count  20% Avg Length  20% Count
Qwen3-4B-Original          406.5       100           463.0        200           539.2        400
Qwen3-4B-ThinkEdit         408.3       100           472.2        200           548.6        400
```

## Key Insights from Example Results

1. **Overall Performance**: ThinkEdit improves accuracy by 0.4% (95.35% vs 94.95%)
2. **Shortest Responses**: Both models achieve near-perfect accuracy (99.75-100%) on shortest reasoning responses
3. **Reasoning Length**: Both models have similar average thinking lengths (~1495 tokens)
4. **Shortest Reasoning**: The shortest 5% of responses average ~407-408 tokens
5. **Consistency**: Both models show similar patterns in terms of reasoning length distribution

## File Format Requirements

The script automatically handles both processed and unprocessed files:

**Unprocessed format** (from `generate_responses_gsm8k.py`):
```json
[
  {
    "id": 0,
    "question": "Question text...",
    "answer": "Ground truth answer...",
    "with_thinking": {
      "thinking": "Model's reasoning process...",
      "response": "Model's final response..."
    }
  }
]
```

**Processed format** (after `postprocess_responses.py`):
```json
[
  {
    "id": 0,
    "question": "Question text...",
    "answer": "Ground truth answer...",
    "with_thinking": {
      "thinking": "Model's reasoning process...",
      "response": "Model's final response...",
      "correct": true,
      "extracted_answer": 42
    }
  }
]
```

## Notes

- The script automatically processes unprocessed files using `postprocess_responses.py`
- Token counting uses the specified tokenizer when available, falls back to character-based approximation
- "Shortest reasoning responses" refers to responses sorted by the number of tokens in the thinking text
- Percentiles are calculated as the top X% shortest responses (e.g., top 5% = shortest 5% of all responses)
- The script handles various thinking text formats including `<think>` tags in full_text fields 